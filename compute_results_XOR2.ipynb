{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "from sklearn.metrics import mean_squared_error as performance_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate XOR2 data\n",
    "\\begin{align*}\n",
    "    y(x,z) \\hspace{0.2cm}  &=\n",
    "    \\begin{cases}\n",
    "        ReLU(0.5{x}_1 + {x}_2),& \\text{if } z = 0,\\\\ \n",
    "       ReLU({x}_1 + 0.5{x}_2) ,& \\text{if } z = 1,\\\\\n",
    "        ReLU(0.5{x}_3 + {x}_4) ,& \\text{if } z = 2,\\\\\n",
    "        ReLU({x}_3 + 0.5{x}_4) ,& \\text{if } z = 3.\\\\\n",
    "    \\end{cases} . \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def get_raw_data(n_samples,data=\"train\",cv_cur=0,dim_x=30):\n",
    "    # Set random seed for reproducibility\n",
    "    if data==\"train\":\n",
    "        np.random.seed(0+10*cv_cur)\n",
    "    elif data==\"val\":\n",
    "        np.random.seed(1+10*cv_cur)\n",
    "    elif data==\"test\":\n",
    "        np.random.seed(2+10*cv_cur)\n",
    "\n",
    "    # Number of samples\n",
    "    # n_samples = 1000\n",
    "\n",
    "    # Dimension of X\n",
    "    dim_x = dim_x\n",
    "\n",
    "    # Generate X\n",
    "    X = np.random.randn(n_samples, dim_x)  # This also covers x4, x5, and x6 as they are Gaussian random noise\n",
    "\n",
    "    # Initialize Y\n",
    "    Y = np.zeros((n_samples, 1))\n",
    "\n",
    "    # Generate Z\n",
    "    Z = np.random.randint(0, 4, (n_samples, 1))\n",
    "\n",
    "    # Compute Y according to Z and the rules\n",
    "    for i in range(n_samples):\n",
    "        if Z[i, 0] == 0:\n",
    "            Y[i, 0] = relu(0.5*X[i, 0] + X[i, 1])\n",
    "        elif Z[i, 0] == 1:\n",
    "            Y[i, 0] = relu(X[i, 0] + 0.5*X[i, 1])\n",
    "        elif Z[i, 0] == 2:\n",
    "            Y[i, 0] = relu(0.5*X[i, 2] + X[i, 3])\n",
    "        elif Z[i, 0] == 3:\n",
    "            Y[i, 0] = relu(X[i, 2] + 0.5*X[i, 3])\n",
    "\n",
    "    # Now, X, Y, and Z are NumPy arrays containing the data.\n",
    "    # X: Input features\n",
    "    # Y: Outputs\n",
    "    # Z: Determines the relationship between X and Y\n",
    "    return X,np.eye(4)[Z.flatten()],Y\n",
    "\n",
    "def get_data(cv_cur,dim_x=30):\n",
    "    \n",
    "    X_train,T_train,y_train = get_raw_data(1000,data=\"train\",cv_cur=cv_cur,dim_x=dim_x)\n",
    "    X_val,T_val,y_val = get_raw_data(1000,data=\"val\",cv_cur=cv_cur,dim_x=dim_x)\n",
    "    X_test,T_test,y_test = get_raw_data(1000,data=\"test\",cv_cur=cv_cur,dim_x=dim_x)\n",
    "    \n",
    "#     X_train,y_train,T_train,X_val,y_val,T_val,X_test,y_test,T_test = get_raw_data(seed=cv_cur)\n",
    "    \n",
    "    input_dim,param_dim,output_dim = X_train.shape[-1],T_train.shape[-1],1\n",
    "    \n",
    "    trainset = data_utils.TensorDataset(torch.tensor(X_train), torch.tensor(y_train[:,None]),torch.tensor(T_train))\n",
    "    valset = data_utils.TensorDataset(torch.tensor(X_val), torch.tensor(y_val[:,None]),torch.tensor(T_val))\n",
    "    testset = data_utils.TensorDataset(torch.tensor(X_test), torch.tensor(y_test[:,None]),torch.tensor(T_test))\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True,drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(valset, batch_size=10000, shuffle=True,drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=True,drop_last=False)\n",
    "    return train_dataloader,val_dataloader,test_dataloader,input_dim,param_dim,output_dim\n",
    "\n",
    "\n",
    "# test_cur = 0\n",
    "# val_cur = 0\n",
    "train_dataloader,val_dataloader,test_dataloader,x_dim,z_dim,y_dim = get_data(0,dim_x=30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(nn.Module):\n",
    "    def __init__(self, hyper_input_dim,hyper_output_dim,hyper_hidden_dim, sigma=0.5,non_param_stg=False,train_sigma=False, is_cen=False):\n",
    "        super(FeatureSelector, self).__init__()\n",
    "        # hyper_input_dim: input dimension for the hyper network i.e dimensionality of contextual input\n",
    "        # hyper_output_dim: output dimension for the hyper network i.e dimensionality of explanatory features\n",
    "        # hyper_hidden_dim: dimensionals of hidden layers in the hyper network\n",
    "        self.non_param_stg = non_param_stg\n",
    "        self.hyper_output_dim = hyper_output_dim\n",
    "        self.train_sigma = train_sigma\n",
    "        self.is_cen = is_cen\n",
    "\n",
    "        # Define hyper network \n",
    "        if self.non_param_stg:\n",
    "            self.mu = torch.nn.Parameter(0.01*torch.randn(self.hyper_output_dim, )+0.5, requires_grad=True)\n",
    "        else:\n",
    "            self.hyper_dense_layers = nn.ModuleList()\n",
    "            if len(hyper_hidden_dim):\n",
    "                self.hyper_dense_layers.append(nn.Linear(hyper_input_dim, hyper_hidden_dim[0]))\n",
    "                self.hyper_dense_layers.append(nn.ReLU())\n",
    "                for i in range(len(hyper_hidden_dim)-1):\n",
    "                    self.hyper_dense_layers.append(nn.Linear(hyper_hidden_dim[i], hyper_hidden_dim[i+1]))\n",
    "                    self.hyper_dense_layers.append(nn.ReLU())\n",
    "                self.hyper_dense_layers.append(nn.Linear(hyper_hidden_dim[-1], hyper_output_dim))\n",
    "                self.hyper_last_weight_layer = nn.Linear(hyper_hidden_dim[-1], hyper_output_dim)\n",
    "            else:\n",
    "                self.hyper_dense_layers.append(nn.Linear(hyper_input_dim, hyper_output_dim))\n",
    "                self.hyper_last_weight_layer = nn.Linear(hyper_input_dim, hyper_output_dim)\n",
    "            self.hyper_dense_layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.noise = torch.randn(hyper_output_dim,) \n",
    "        self.sigma = nn.Parameter(torch.tensor([sigma]), requires_grad=train_sigma)\n",
    "\n",
    "    def forward(self, prev_x, B, axis=2):\n",
    "        # compute the feature importance given B\n",
    "        stochastic_gate, weights = self.get_feature_importance(B)\n",
    "        \n",
    "        # mask the input with feature importance\n",
    "        if self.non_param_stg:\n",
    "            new_x = prev_x * stochastic_gate[None,:]\n",
    "        else:\n",
    "            new_x = prev_x * stochastic_gate[:,:]\n",
    "            new_x = new_x * weights\n",
    "        return new_x\n",
    "    \n",
    "    def get_feature_importance(self,B=None):\n",
    "        # compute feature importance given contextual input (B)\n",
    "        if not self.non_param_stg:\n",
    "            self.mu = B\n",
    "            self.weights = B\n",
    "            for layer_idx,dense in enumerate(self.hyper_dense_layers):\n",
    "                self.mu = dense(self.mu)\n",
    "                if layer_idx<=len(self.hyper_dense_layers)-3:\n",
    "                    self.weights = dense(self.weights)\n",
    "                elif layer_idx==len(self.hyper_dense_layers)-2:\n",
    "                    self.weights = self.hyper_last_weight_layer(self.weights)\n",
    "                    \n",
    "        if self.train_sigma:\n",
    "            self.sigma = nn.ReLU(self.sigma)+0.01\n",
    "            \n",
    "        if self.is_cen:\n",
    "            stochastic_gate = self.mu\n",
    "            self.weights = None\n",
    "        else:\n",
    "            z = self.mu + (self.sigma)*self.noise.normal_()*self.training \n",
    "            stochastic_gate = self.hard_sigmoid(z)\n",
    "\n",
    "        return stochastic_gate,self.weights\n",
    "        \n",
    "    \n",
    "    def hard_sigmoid(self, x):\n",
    "        return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "    def regularizer(self, x):\n",
    "        ''' Gaussian CDF. '''\n",
    "        return 0.5 * (1 + torch.erf(x / self.sigma*math.sqrt(2))) \n",
    "\n",
    "    def _apply(self, fn):\n",
    "        super(FeatureSelector, self)._apply(fn)\n",
    "        self.noise = fn(self.noise)\n",
    "        return self\n",
    "\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, x_dim, z_dim, y_dim,hyper_hidden_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.y_dim = y_dim\n",
    "        \n",
    "        self.gates = FeatureSelector(z_dim, x_dim, hyper_hidden_dim)\n",
    "        self.reg = self.gates.regularizer \n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        x = self.gates(x, z)\n",
    "        out = torch.sum(x , dim=1)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained models and Compute test r2_score and sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9956 (0.0008)\n",
      "Sparsity: 2.00 (2.00)\n"
     ]
    }
   ],
   "source": [
    "ML_model_name = \"cSTG_param_prediction_model\"\n",
    "\n",
    "epochs = 500\n",
    "dropout = 0\n",
    "\n",
    "dim_x = 25\n",
    "learning_rate = 0.01\n",
    "stg_regularizer = 0.01\n",
    "rand_init_seed = 0\n",
    "input_dim = 25\n",
    "param_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "gpu = torch.device('cpu')\n",
    "\n",
    "hyper_hidden_dim = []\n",
    "hidden_dims = []\n",
    "\n",
    "\n",
    "train_metric_rand_init = {}\n",
    "val_metric_rand_init = {}\n",
    "test_metric_rand_init = {}\n",
    "sparsity_rand_init = {}\n",
    "clf_stats = []\n",
    "\n",
    "dim_x = 25\n",
    "learning_rate = 0.01\n",
    "stg_regularizer = 0.01\n",
    "rand_init_seed = 0\n",
    "stg = True \n",
    "train_metric_rand_init[rand_init_seed] = []   \n",
    "val_metric_rand_init[rand_init_seed] = []\n",
    "test_metric_rand_init[rand_init_seed] = []\n",
    "sparsity_rand_init[rand_init_seed] = []\n",
    "\n",
    "\n",
    "for val_cur in range(5):\n",
    "\n",
    "\n",
    "    add_foldername = \"\"\n",
    "\n",
    "\n",
    "    root_fname = \"./Trained_Models/XOR2\"\n",
    "          \n",
    "    if not os.path.exists(root_fname):\n",
    "        os.mkdir(root_fname)\n",
    "        \n",
    "    train_dataloader,val_dataloader,test_dataloader,input_dim,param_dim,output_dim = get_data(val_cur,dim_x=dim_x)\n",
    "    x_dim,z_dim,y_dim = input_dim,param_dim,output_dim\n",
    "\n",
    "    torch.manual_seed(rand_init_seed)\n",
    "    np.random.seed(rand_init_seed)\n",
    "\n",
    "    add_name = \"\"\n",
    "    add_name += \"_\"+\"_\".join(np.array([input_dim]+hidden_dims+[output_dim]).astype(str))\n",
    "\n",
    "    add_name += \"_hyper_\"+\"_\".join(np.array([param_dim]+hyper_hidden_dim+[input_dim]).astype(str))\n",
    "\n",
    "    model_path = \"{}/{}_lr_{}_stg_lr_{}{}_init_{}_val_seed_{}_stg_{}.model\".format(root_fname,ML_model_name,str(learning_rate).replace(\".\", \"_\"),str(stg_regularizer).replace(\".\",\"_\"),add_name,val_cur,rand_init_seed,stg)\n",
    "    loss_path = model_path.replace(\"model\",\"mat\")\n",
    "    \n",
    "    model = LinearModel(x_dim, z_dim, y_dim, hyper_hidden_dim)\n",
    "    model = model.to(gpu).float()\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=gpu))\n",
    "    model.eval()\n",
    "\n",
    "    mat_content = scipy.io.loadmat(loss_path)\n",
    "    val_metric_array = np.squeeze(mat_content['val_metric'], axis=0).tolist()[1:]\n",
    "    test_metric_array = np.squeeze(mat_content['test_metric'], axis=0).tolist()[1:]\n",
    "\n",
    "    val_metric = val_metric_array[np.argmax(val_metric_array)]\n",
    "    test_metric = test_metric_array[np.argmax(val_metric_array)]\n",
    "\n",
    "    val_metric_rand_init[rand_init_seed].append(val_metric)\n",
    "    test_metric_rand_init[rand_init_seed].append(test_metric)\n",
    "\n",
    "    sparsity = 0\n",
    "    count = 0\n",
    "    for batch, (input, target, B) in enumerate(test_dataloader):\n",
    "        input = input.to(gpu).float()\n",
    "        target = target.to(gpu).float()\n",
    "        B = B.to(gpu).float()\n",
    "        mu,w = model.gates.get_feature_importance(B)\n",
    "        mu = mu.detach().cpu().numpy()\n",
    "        w = w.detach().cpu().numpy()\n",
    "\n",
    "        sparsity += np.sum(np.sum(((mu>0.6)*w)!=0,axis=1))\n",
    "        count += mu.shape[0]\n",
    "    sparsity = sparsity/count\n",
    "\n",
    "    sparsity_rand_init[rand_init_seed].append(sparsity)\n",
    "\n",
    "\n",
    "val_metric_mean = np.round(np.mean(val_metric_rand_init[rand_init_seed]),4)\n",
    "test_metric_mean = np.round(np.mean(test_metric_rand_init[rand_init_seed]),4)\n",
    "val_metric_std = np.round(np.std(val_metric_rand_init[rand_init_seed]),4)\n",
    "test_metric_std = np.round(np.std(test_metric_rand_init[rand_init_seed]),4)\n",
    "\n",
    "sparsity_mean = np.round(np.mean(sparsity_rand_init[rand_init_seed]),4)\n",
    "sparsity_std = np.round(np.std(sparsity_rand_init[rand_init_seed]),4)\n",
    "\n",
    "print(\"Test Accuracy: {:.4f} ({:.4f})\".format(test_metric_mean,test_metric_std))\n",
    "print(\"Sparsity: {:.2f} ({:.2f})\".format(sparsity_mean,sparsity_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot feature significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim_x = 25\n",
    "learning_rate = 0.01\n",
    "stg_regularizer = 0.01\n",
    "rand_init_seed = 0\n",
    "\n",
    "val_cur = 0\n",
    "\n",
    "train_dataloader,val_dataloader,test_dataloader,input_dim,param_dim,output_dim = get_data(val_cur,dim_x=dim_x)\n",
    "x_dim,z_dim,y_dim = input_dim,param_dim,output_dim\n",
    "\n",
    "torch.manual_seed(rand_init_seed)\n",
    "np.random.seed(rand_init_seed)\n",
    "\n",
    "add_foldername = \"\"\n",
    "\n",
    "root_fname = \"./Trained_Models/XOR2\"\n",
    "if not os.path.exists(root_fname):\n",
    "    os.mkdir(root_fname)\n",
    "\n",
    "add_name = \"\"\n",
    "add_name += \"_\"+\"_\".join(np.array([input_dim]+hidden_dims+[output_dim]).astype(str))\n",
    "\n",
    "add_name += \"_hyper_\"+\"_\".join(np.array([param_dim]+hyper_hidden_dim+[input_dim]).astype(str))\n",
    "\n",
    "model_path = \"{}/{}_lr_{}_stg_lr_{}{}_init_{}_val_seed_{}_stg_{}.model\".format(root_fname,ML_model_name,str(learning_rate).replace(\".\", \"_\"),str(stg_regularizer).replace(\".\",\"_\"),add_name,val_cur,rand_init_seed,stg)\n",
    "loss_path = model_path.replace(\"model\",\"mat\")\n",
    "\n",
    "model = LinearModel(x_dim, z_dim, y_dim, hyper_hidden_dim)\n",
    "model = model.to(gpu).float()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "z = torch.tensor(np.eye(4)).to(gpu).to(B.dtype)\n",
    "mu,w = model.gates.get_feature_importance(z)\n",
    "mu = mu.detach().cpu().numpy()\n",
    "w = w.detach().cpu().numpy()\n",
    "\n",
    "mu = mu>0.5\n",
    "\n",
    "w_cSTG_feature_significance = w*mu\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.imshow(w_cSTG_feature_significance,vmin=0,vmax=1,cmap='gray')\n",
    "plt.yticks([0,1,2,3],fontsize=18)\n",
    "plt.ylabel(\"Context\",fontsize=18)\n",
    "plt.xticks([])\n",
    "plt.title(\"Weighted c-STG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
